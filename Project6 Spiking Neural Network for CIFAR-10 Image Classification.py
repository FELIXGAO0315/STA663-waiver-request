"""
RTX 5070 Tiå¯ç”¨ç‰ˆSNN - CIFAR-10ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰
"""
import sys
import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import math
import numpy as np
import argparse
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix


# ===================== åˆå§‹åŒ–ï¼šåˆ›å»ºchartsæ–‡ä»¶å¤¹ =====================
os.makedirs('./charts', exist_ok=True)
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['font.sans-serif'] = ['SimHei']  # Windowsä¸­æ–‡æ”¯æŒ
plt.rcParams['axes.unicode_minus'] = False


# ===================== è®¾å¤‡è®¾ç½® =====================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"âœ… GPUå‹å·: {torch.cuda.get_device_name(0)}")
    print(f"âœ… GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB")
    cudnn.benchmark = True
    cudnn.enabled = True

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'


# ===================== å‚æ•°è§£æ =====================
parser = argparse.ArgumentParser(description='RTX 5070 Ti SNN CIFAR10 (Final Working Version)')
parser.add_argument('data', metavar='DIR', help='path to dataset')
parser.add_argument('-b', '--batch-size', default=32, type=int)
parser.add_argument('-T', '--timesteps', default=10, type=int)
parser.add_argument('--lr', '--learning-rate', default=0.001, type=float)
parser.add_argument('--epochs', default=10, type=int)
args = parser.parse_args()


# ===================== æ­£ç¡®çš„SNNç¥ç»å…ƒï¼ˆè†œç”µä½ä¸ºåŠ¨æ€çŠ¶æ€ï¼Œä¸å‚ä¸è®­ç»ƒï¼‰ =====================
class IFNode(nn.Module):
    def __init__(self, v_threshold=0.5, v_reset=0.0):
        super().__init__()
        self.v_threshold = v_threshold
        self.v_reset = v_reset
        # è†œç”µä½æ˜¯åŠ¨æ€çŠ¶æ€ï¼Œæ¯æ¬¡å‰å‘é‡æ–°åˆå§‹åŒ–ï¼ˆä¿ç•™æ¢¯åº¦ï¼‰
        self.v = None

    def forward(self, dv: torch.Tensor):
        # åˆå§‹åŒ–è†œç”µä½ï¼ˆä¸è¾“å…¥åŒå½¢çŠ¶ã€åŒè®¾å¤‡ï¼Œä¿ç•™æ¢¯åº¦ï¼‰
        if self.v is None:
            self.v = torch.full_like(dv, self.v_reset, device=device, requires_grad=True)
        
        # è†œç”µä½ç´¯ç§¯ï¼ˆä¿ç•™æ¢¯åº¦ï¼‰
        self.v = self.v + dv
        # è„‰å†²å‘æ”¾ï¼ˆç”¨å¯å¾®åˆ†çš„è¿‘ä¼¼é˜¶è·ƒå‡½æ•°ï¼‰
        spike = torch.sigmoid(10 * (self.v - self.v_threshold))  # è¿‘ä¼¼é˜¶è·ƒï¼Œç¡®ä¿æ¢¯åº¦
        # è†œç”µä½é‡ç½®ï¼ˆéin-placeæ“ä½œï¼Œä¿ç•™æ¢¯åº¦ï¼‰
        self.v = torch.where(spike > 0.5, torch.tensor(self.v_reset, device=device), self.v)
        return spike

    def reset(self):
        # é‡ç½®è†œç”µä½ï¼ˆè®­ç»ƒä¸‹ä¸€ä¸ªbatchå‰æ¸…ç©ºï¼‰
        self.v = None


# ===================== è½»é‡SNNæ¨¡å‹ =====================
class LightSNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            # å·ç§¯å±‚1
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),
            IFNode(),
            nn.MaxPool2d(2),
            # å·ç§¯å±‚2
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),
            IFNode(),
            nn.MaxPool2d(2),
            # å·ç§¯å±‚3
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),
            IFNode(),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.Linear(64 * 4 * 4, 128, bias=False),
            IFNode(),
            nn.Linear(128, 10, bias=False),
            IFNode()
        )

        # åˆå§‹åŒ–æƒé‡
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

    def reset_(self):
        # é‡ç½®æ‰€æœ‰ç¥ç»å…ƒçš„è†œç”µä½
        for module in self.modules():
            if isinstance(module, IFNode):
                module.reset()


# ===================== å›¾è¡¨ç”Ÿæˆå‡½æ•° =====================
def plot_accuracy_curve(epochs, train_accs, test_accs):
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_accs, label='è®­ç»ƒç²¾åº¦', linewidth=2.5, marker='o', markersize=6)
    plt.plot(epochs, test_accs, label='æµ‹è¯•ç²¾åº¦', linewidth=2.5, marker='s', markersize=6, color='red')
    plt.xlabel('è®­ç»ƒè½®æ¬¡ï¼ˆEpochï¼‰', fontsize=12)
    plt.ylabel('ç²¾åº¦ï¼ˆ%ï¼‰', fontsize=12)
    plt.title('SNNæ¨¡å‹è®­ç»ƒ/æµ‹è¯•ç²¾åº¦å˜åŒ–æ›²çº¿ï¼ˆRTX 5070 Tiï¼‰', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.ylim(0, 100)
    plt.savefig('./charts/accuracy_curve.png', dpi=300, bbox_inches='tight')
    plt.close()


def plot_loss_curve(epochs, train_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_losses, label='è®­ç»ƒæŸå¤±', linewidth=2.5, marker='o', color='orange')
    plt.xlabel('è®­ç»ƒè½®æ¬¡ï¼ˆEpochï¼‰', fontsize=12)
    plt.ylabel('æŸå¤±å€¼', fontsize=12)
    plt.title('SNNæ¨¡å‹è®­ç»ƒæŸå¤±å˜åŒ–æ›²çº¿ï¼ˆRTX 5070 Tiï¼‰', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.savefig('./charts/loss_curve.png', dpi=300, bbox_inches='tight')
    plt.close()


def plot_spike_heatmap(net, test_img, T):
    net.eval()
    net.reset_()
    spike_records = []
    with torch.no_grad():
        for t in range(T):
            spike = net.features[1](net.features[0](test_img))  # ç¬¬ä¸€å±‚ç¥ç»å…ƒè„‰å†²
            spike_records.append(spike.cpu().numpy()[0, :50])  # å–ç¬¬1ä¸ªæ ·æœ¬çš„å‰50ä¸ªç¥ç»å…ƒ
            net.reset_()  # é‡ç½®å½“å‰å±‚ç¥ç»å…ƒ
    
    plt.figure(figsize=(12, 6))
    sns.heatmap(np.stack(spike_records), cmap='binary', cbar_kws={'label': 'è„‰å†²å‘æ”¾ï¼ˆ1=å‘æ”¾ï¼Œ0=æœªå‘æ”¾ï¼‰'})
    plt.xlabel('ç¥ç»å…ƒç¼–å·', fontsize=12)
    plt.ylabel('æ—¶é—´æ­¥ï¼ˆTimestepï¼‰', fontsize=12)
    plt.title('SNNç¬¬ä¸€å±‚ç¥ç»å…ƒè„‰å†²å‘æ”¾çƒ­åŠ›å›¾ï¼ˆT=10ï¼‰', fontsize=14, fontweight='bold')
    plt.savefig('./charts/spike_heatmap.png', dpi=300, bbox_inches='tight')
    plt.close()


def plot_confusion_matrix(all_labels, all_preds):
    class_names = ['é£æœº', 'æ±½è½¦', 'é¸Ÿ', 'çŒ«', 'é¹¿', 'ç‹—', 'é’è›™', 'é©¬', 'èˆ¹', 'å¡è½¦']
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    plt.ylabel('çœŸå®ç±»åˆ«', fontsize=12)
    plt.title('SNNæ¨¡å‹CIFAR-10æ··æ·†çŸ©é˜µï¼ˆæµ‹è¯•é›†ï¼‰', fontsize=14, fontweight='bold')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.savefig('./charts/confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.close()


# ===================== ä¸»è®­ç»ƒå‡½æ•° =====================
def main():
    # æ•°æ®åŠ è½½
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    train_loader = torch.utils.data.DataLoader(
        torchvision.datasets.CIFAR10(root=args.data, train=True, download=True, transform=transform_train),
        batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True
    )
    test_loader = torch.utils.data.DataLoader(
        torchvision.datasets.CIFAR10(root=args.data, train=False, download=True, transform=transform_test),
        batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True
    )

    # æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨
    model = LightSNN().to(device)
    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)

    # è®­ç»ƒè®°å½•
    train_accs = []
    test_accs = []
    train_losses = []
    start_time = time.time()

    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Train]')
        for inputs, labels in pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            model.reset_()  # é‡ç½®æ‰€æœ‰ç¥ç»å…ƒ

            # SNNå‰å‘ï¼ˆç´¯ç§¯æ—¶é—´æ­¥è„‰å†²ï¼‰
            output = torch.zeros((inputs.shape[0], 10), device=device)
            for t in range(args.timesteps):
                output += model(inputs)
                model.reset_()  # é‡ç½®ç¥ç»å…ƒï¼Œå‡†å¤‡ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥

            # è®¡ç®—æŸå¤±ä¸æ¢¯åº¦
            loss = criterion(output / args.timesteps, labels)
            loss.backward()
            optimizer.step()

            # ç»Ÿè®¡
            train_loss += loss.item() * inputs.size(0)
            _, predicted = output.max(1)
            train_total += labels.size(0)
            train_correct += predicted.eq(labels).sum().item()
            pbar.set_postfix(acc=f"{100*train_correct/train_total:.2f}%", loss=f"{train_loss/train_total:.4f}")

        # æµ‹è¯•
        model.eval()
        test_correct = 0
        test_total = 0
        all_labels = []
        all_preds = []
        with torch.no_grad():
            pbar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{args.epochs} [Test]')
            for inputs, labels in pbar:
                inputs, labels = inputs.to(device), labels.to(device)
                model.reset_()
                output = torch.zeros((inputs.shape[0], 10), device=device)
                for t in range(args.timesteps):
                    output += model(inputs)
                    model.reset_()
                _, predicted = output.max(1)
                test_total += labels.size(0)
                test_correct += predicted.eq(labels).sum().item()
                all_labels.extend(labels.cpu().numpy())
                all_preds.extend(predicted.cpu().numpy())
                pbar.set_postfix(acc=f"{100*test_correct/test_total:.2f}%")

        # è®°å½•
        train_acc = 100 * train_correct / train_total
        test_acc = 100 * test_correct / test_total
        train_loss_avg = train_loss / train_total
        train_accs.append(train_acc)
        test_accs.append(test_acc)
        train_losses.append(train_loss_avg)
        scheduler.step()

        # æ‰“å°
        print(f"\nEpoch {epoch+1} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}% | Loss: {train_loss_avg:.4f}")

    # ç”Ÿæˆå›¾è¡¨
    plot_accuracy_curve(list(range(1, args.epochs+1)), train_accs, test_accs)
    plot_loss_curve(list(range(1, args.epochs+1)), train_losses)
    plot_confusion_matrix(all_labels, all_preds)
    # ç”Ÿæˆè„‰å†²çƒ­åŠ›å›¾ï¼ˆå–æµ‹è¯•é›†ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
    test_img = next(iter(test_loader))[0][0:1].to(device)
    plot_spike_heatmap(model, test_img, args.timesteps)

    print(f"\nè®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {(time.time()-start_time)/60:.2f} åˆ†é’Ÿ | æœ€ä½³æµ‹è¯•ç²¾åº¦: {max(test_accs):.2f}%")
    print("æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ° ./charts æ–‡ä»¶å¤¹")


if __name__ == '__main__':
    main()